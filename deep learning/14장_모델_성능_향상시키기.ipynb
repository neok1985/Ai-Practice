{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14장 모델 성능 향상시키기.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "5v18tmhyZjBI",
        "outputId": "35a07d9a-d2ad-46f0-f890-ba80835abbae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 21 (delta 3), reused 20 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0     1     2    3      4     5     6       7     8     9    10  11  12\n",
              "0   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1\n",
              "1   7.8  0.88  0.00  2.6  0.098  25.0  67.0  0.9968  3.20  0.68  9.8   5   1\n",
              "2   7.8  0.76  0.04  2.3  0.092  15.0  54.0  0.9970  3.26  0.65  9.8   5   1\n",
              "3  11.2  0.28  0.56  1.9  0.075  17.0  60.0  0.9980  3.16  0.58  9.8   6   1\n",
              "4   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b35e1096-40a2-428a-a07a-53ab398a9f13\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b35e1096-40a2-428a-a07a-53ab398a9f13')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b35e1096-40a2-428a-a07a-53ab398a9f13 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b35e1096-40a2-428a-a07a-53ab398a9f13');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "df=pd.read_csv('./data/wine.csv',header=None)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.iloc[:,0:12]\n",
        "y=df.iloc[:,12]"
      ],
      "metadata": {
        "id": "83SteJKiakIB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30,input_dim=12,activation='relu'))\n",
        "model.add(Dense(12,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RAhJOKXa1VC",
        "outputId": "40230fa0-73b0-4eb4-fb5a-841d3710fc9c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 30)                390       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 12)                372       \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 875\n",
            "Trainable params: 875\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "# history=model.fit(X_train,y_train,epochs=50,batch_size=500,validation_split=0.25)"
      ],
      "metadata": {
        "id": "NfvgWBFZd974"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score=model.evaluate(X_test,y_test)\n",
        "print('Test accuracy:',score[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YykUu_ZhenE1",
        "outputId": "ea625bc4-ebfe-4e40-f284-5131e2dde646"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1409 - accuracy: 0.9446\n",
            "Test accuracy: 0.944615364074707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath='./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5'"
      ],
      "metadata": {
        "id": "1lvhRzoXgg-T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpointer=ModelCheckpoint(filepath=modelpath,verbose=1)\n",
        "history=model.fit(X_train,y_train,epochs=50,batch_size=500,validation_split=0.25,verbose=0,callbacks=[checkpointer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u7aYeCHhQGD",
        "outputId": "0b1bd015-018d-429f-9a11-fa1553d19236"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: saving model to ./data/model/all/01-0.9492.hdf5\n",
            "\n",
            "Epoch 2: saving model to ./data/model/all/02-0.9538.hdf5\n",
            "\n",
            "Epoch 3: saving model to ./data/model/all/03-0.9515.hdf5\n",
            "\n",
            "Epoch 4: saving model to ./data/model/all/04-0.9515.hdf5\n",
            "\n",
            "Epoch 5: saving model to ./data/model/all/05-0.9538.hdf5\n",
            "\n",
            "Epoch 6: saving model to ./data/model/all/06-0.9538.hdf5\n",
            "\n",
            "Epoch 7: saving model to ./data/model/all/07-0.9538.hdf5\n",
            "\n",
            "Epoch 8: saving model to ./data/model/all/08-0.9554.hdf5\n",
            "\n",
            "Epoch 9: saving model to ./data/model/all/09-0.9554.hdf5\n",
            "\n",
            "Epoch 10: saving model to ./data/model/all/10-0.9531.hdf5\n",
            "\n",
            "Epoch 11: saving model to ./data/model/all/11-0.9592.hdf5\n",
            "\n",
            "Epoch 12: saving model to ./data/model/all/12-0.9562.hdf5\n",
            "\n",
            "Epoch 13: saving model to ./data/model/all/13-0.9569.hdf5\n",
            "\n",
            "Epoch 14: saving model to ./data/model/all/14-0.9554.hdf5\n",
            "\n",
            "Epoch 15: saving model to ./data/model/all/15-0.9585.hdf5\n",
            "\n",
            "Epoch 16: saving model to ./data/model/all/16-0.9608.hdf5\n",
            "\n",
            "Epoch 17: saving model to ./data/model/all/17-0.9592.hdf5\n",
            "\n",
            "Epoch 18: saving model to ./data/model/all/18-0.9615.hdf5\n",
            "\n",
            "Epoch 19: saving model to ./data/model/all/19-0.9608.hdf5\n",
            "\n",
            "Epoch 20: saving model to ./data/model/all/20-0.9562.hdf5\n",
            "\n",
            "Epoch 21: saving model to ./data/model/all/21-0.9615.hdf5\n",
            "\n",
            "Epoch 22: saving model to ./data/model/all/22-0.9600.hdf5\n",
            "\n",
            "Epoch 23: saving model to ./data/model/all/23-0.9577.hdf5\n",
            "\n",
            "Epoch 24: saving model to ./data/model/all/24-0.9638.hdf5\n",
            "\n",
            "Epoch 25: saving model to ./data/model/all/25-0.9638.hdf5\n",
            "\n",
            "Epoch 26: saving model to ./data/model/all/26-0.9615.hdf5\n",
            "\n",
            "Epoch 27: saving model to ./data/model/all/27-0.9615.hdf5\n",
            "\n",
            "Epoch 28: saving model to ./data/model/all/28-0.9646.hdf5\n",
            "\n",
            "Epoch 29: saving model to ./data/model/all/29-0.9662.hdf5\n",
            "\n",
            "Epoch 30: saving model to ./data/model/all/30-0.9654.hdf5\n",
            "\n",
            "Epoch 31: saving model to ./data/model/all/31-0.9677.hdf5\n",
            "\n",
            "Epoch 32: saving model to ./data/model/all/32-0.9646.hdf5\n",
            "\n",
            "Epoch 33: saving model to ./data/model/all/33-0.9669.hdf5\n",
            "\n",
            "Epoch 34: saving model to ./data/model/all/34-0.9662.hdf5\n",
            "\n",
            "Epoch 35: saving model to ./data/model/all/35-0.9654.hdf5\n",
            "\n",
            "Epoch 36: saving model to ./data/model/all/36-0.9638.hdf5\n",
            "\n",
            "Epoch 37: saving model to ./data/model/all/37-0.9631.hdf5\n",
            "\n",
            "Epoch 38: saving model to ./data/model/all/38-0.9677.hdf5\n",
            "\n",
            "Epoch 39: saving model to ./data/model/all/39-0.9662.hdf5\n",
            "\n",
            "Epoch 40: saving model to ./data/model/all/40-0.9669.hdf5\n",
            "\n",
            "Epoch 41: saving model to ./data/model/all/41-0.9685.hdf5\n",
            "\n",
            "Epoch 42: saving model to ./data/model/all/42-0.9692.hdf5\n",
            "\n",
            "Epoch 43: saving model to ./data/model/all/43-0.9692.hdf5\n",
            "\n",
            "Epoch 44: saving model to ./data/model/all/44-0.9669.hdf5\n",
            "\n",
            "Epoch 45: saving model to ./data/model/all/45-0.9685.hdf5\n",
            "\n",
            "Epoch 46: saving model to ./data/model/all/46-0.9662.hdf5\n",
            "\n",
            "Epoch 47: saving model to ./data/model/all/47-0.9685.hdf5\n",
            "\n",
            "Epoch 48: saving model to ./data/model/all/48-0.9692.hdf5\n",
            "\n",
            "Epoch 49: saving model to ./data/model/all/49-0.9692.hdf5\n",
            "\n",
            "Epoch 50: saving model to ./data/model/all/50-0.9662.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score=model.evaluate(X_test,y_test)\n",
        "print('Test accuracy:',score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mej7HbUiCWQ",
        "outputId": "40a3c08a-40c2-4385-b60a-10fa1fa78ef1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9631\n",
            "Test accuracy: 0.9630769491195679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "ykQtcfDsk04u"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(X_train,y_train,epochs=2000,batch_size=500,validation_split=0.25,verbose=0)"
      ],
      "metadata": {
        "id": "Hvf1M1Sfi85D"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist_df=pd.DataFrame(history.history)\n",
        "hist_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DxSJZm2mjdog",
        "outputId": "e7fea3b9-45d8-48ad-a17c-3a273fe86e0a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         loss  accuracy  val_loss  val_accuracy\n",
              "0    6.585190  0.246343  3.729342      0.258462\n",
              "1    2.026740  0.351039  0.493596      0.686154\n",
              "2    0.383956  0.832435  0.397909      0.845385\n",
              "3    0.422083  0.856043  0.440077      0.848462\n",
              "4    0.416077  0.868104  0.393960      0.867692\n",
              "..        ...       ...       ...           ...\n",
              "255  0.048480  0.986400  0.059210      0.980769\n",
              "256  0.046419  0.987426  0.064887      0.976923\n",
              "257  0.047349  0.986400  0.057097      0.983077\n",
              "258  0.046568  0.986400  0.060755      0.980000\n",
              "259  0.047736  0.988453  0.063015      0.978462\n",
              "\n",
              "[260 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67a4c1e2-4789-4806-80a7-f76ec647049f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.585190</td>\n",
              "      <td>0.246343</td>\n",
              "      <td>3.729342</td>\n",
              "      <td>0.258462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.026740</td>\n",
              "      <td>0.351039</td>\n",
              "      <td>0.493596</td>\n",
              "      <td>0.686154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.383956</td>\n",
              "      <td>0.832435</td>\n",
              "      <td>0.397909</td>\n",
              "      <td>0.845385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.422083</td>\n",
              "      <td>0.856043</td>\n",
              "      <td>0.440077</td>\n",
              "      <td>0.848462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.416077</td>\n",
              "      <td>0.868104</td>\n",
              "      <td>0.393960</td>\n",
              "      <td>0.867692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>0.048480</td>\n",
              "      <td>0.986400</td>\n",
              "      <td>0.059210</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>0.046419</td>\n",
              "      <td>0.987426</td>\n",
              "      <td>0.064887</td>\n",
              "      <td>0.976923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>0.047349</td>\n",
              "      <td>0.986400</td>\n",
              "      <td>0.057097</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>0.046568</td>\n",
              "      <td>0.986400</td>\n",
              "      <td>0.060755</td>\n",
              "      <td>0.980000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>0.047736</td>\n",
              "      <td>0.988453</td>\n",
              "      <td>0.063015</td>\n",
              "      <td>0.978462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>260 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67a4c1e2-4789-4806-80a7-f76ec647049f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67a4c1e2-4789-4806-80a7-f76ec647049f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67a4c1e2-4789-4806-80a7-f76ec647049f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_vloss=hist_df['val_loss']\n",
        "y_loss=hist_df['loss']"
      ],
      "metadata": {
        "id": "4wnO0zu_jpgC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "x_len=np.arange(len(y_loss))\n",
        "plt.plot(x_len,y_vloss,'o',c='red',markersize=2,label='Testset_loss')\n",
        "plt.plot(x_len,y_loss,'o',c='blue',markersize=2,label='Trainset_loss')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2mynep54j0TJ",
        "outputId": "c3fe7ff1-fc13-4573-9136-99d90ddcee31"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "modelpath='./data/early_stop.hdf5'\n",
        "early_stopping_callback=EarlyStopping(monitor='val_loss',patience=20)\n",
        "checkpointer=ModelCheckpoint(filepath=modelpath,verbose=0,monitor='val_loss',save_best_only=True)\n",
        "history=model.fit(X_train,y_train,epochs=2000,batch_size=500,validation_split=0.25,verbose=1,callbacks=[early_stopping_callback,checkpointer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Xm163ym-5a",
        "outputId": "6899d01e-6ead-4eca-f790-910c8a52715d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "8/8 [==============================] - 1s 34ms/step - loss: 6.5852 - accuracy: 0.2463 - val_loss: 3.7293 - val_accuracy: 0.2585\n",
            "Epoch 2/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.0267 - accuracy: 0.3510 - val_loss: 0.4936 - val_accuracy: 0.6862\n",
            "Epoch 3/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3840 - accuracy: 0.8324 - val_loss: 0.3979 - val_accuracy: 0.8454\n",
            "Epoch 4/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4221 - accuracy: 0.8560 - val_loss: 0.4401 - val_accuracy: 0.8485\n",
            "Epoch 5/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4161 - accuracy: 0.8681 - val_loss: 0.3940 - val_accuracy: 0.8677\n",
            "Epoch 6/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3562 - accuracy: 0.8904 - val_loss: 0.3220 - val_accuracy: 0.8938\n",
            "Epoch 7/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2870 - accuracy: 0.9086 - val_loss: 0.2599 - val_accuracy: 0.9162\n",
            "Epoch 8/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2443 - accuracy: 0.9217 - val_loss: 0.2325 - val_accuracy: 0.9238\n",
            "Epoch 9/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2309 - accuracy: 0.9276 - val_loss: 0.2292 - val_accuracy: 0.9231\n",
            "Epoch 10/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2239 - accuracy: 0.9258 - val_loss: 0.2229 - val_accuracy: 0.9231\n",
            "Epoch 11/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2156 - accuracy: 0.9294 - val_loss: 0.2173 - val_accuracy: 0.9246\n",
            "Epoch 12/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2091 - accuracy: 0.9305 - val_loss: 0.2097 - val_accuracy: 0.9254\n",
            "Epoch 13/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2028 - accuracy: 0.9317 - val_loss: 0.2031 - val_accuracy: 0.9254\n",
            "Epoch 14/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1985 - accuracy: 0.9353 - val_loss: 0.2012 - val_accuracy: 0.9238\n",
            "Epoch 15/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1966 - accuracy: 0.9353 - val_loss: 0.2004 - val_accuracy: 0.9231\n",
            "Epoch 16/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1948 - accuracy: 0.9356 - val_loss: 0.1966 - val_accuracy: 0.9269\n",
            "Epoch 17/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1931 - accuracy: 0.9364 - val_loss: 0.1947 - val_accuracy: 0.9269\n",
            "Epoch 18/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1915 - accuracy: 0.9374 - val_loss: 0.1930 - val_accuracy: 0.9269\n",
            "Epoch 19/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1902 - accuracy: 0.9374 - val_loss: 0.1917 - val_accuracy: 0.9262\n",
            "Epoch 20/2000\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1888 - accuracy: 0.9376 - val_loss: 0.1897 - val_accuracy: 0.9285\n",
            "Epoch 21/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1878 - accuracy: 0.9376 - val_loss: 0.1890 - val_accuracy: 0.9262\n",
            "Epoch 22/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1867 - accuracy: 0.9379 - val_loss: 0.1883 - val_accuracy: 0.9277\n",
            "Epoch 23/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1853 - accuracy: 0.9376 - val_loss: 0.1861 - val_accuracy: 0.9277\n",
            "Epoch 24/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1845 - accuracy: 0.9376 - val_loss: 0.1855 - val_accuracy: 0.9285\n",
            "Epoch 25/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1833 - accuracy: 0.9392 - val_loss: 0.1837 - val_accuracy: 0.9285\n",
            "Epoch 26/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1823 - accuracy: 0.9392 - val_loss: 0.1830 - val_accuracy: 0.9292\n",
            "Epoch 27/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1810 - accuracy: 0.9392 - val_loss: 0.1813 - val_accuracy: 0.9323\n",
            "Epoch 28/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1801 - accuracy: 0.9387 - val_loss: 0.1806 - val_accuracy: 0.9315\n",
            "Epoch 29/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1788 - accuracy: 0.9389 - val_loss: 0.1793 - val_accuracy: 0.9300\n",
            "Epoch 30/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1780 - accuracy: 0.9394 - val_loss: 0.1778 - val_accuracy: 0.9323\n",
            "Epoch 31/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1767 - accuracy: 0.9397 - val_loss: 0.1776 - val_accuracy: 0.9331\n",
            "Epoch 32/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1753 - accuracy: 0.9405 - val_loss: 0.1766 - val_accuracy: 0.9308\n",
            "Epoch 33/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1741 - accuracy: 0.9405 - val_loss: 0.1745 - val_accuracy: 0.9346\n",
            "Epoch 34/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1730 - accuracy: 0.9405 - val_loss: 0.1735 - val_accuracy: 0.9354\n",
            "Epoch 35/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1717 - accuracy: 0.9400 - val_loss: 0.1730 - val_accuracy: 0.9362\n",
            "Epoch 36/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1718 - accuracy: 0.9410 - val_loss: 0.1721 - val_accuracy: 0.9362\n",
            "Epoch 37/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1696 - accuracy: 0.9410 - val_loss: 0.1699 - val_accuracy: 0.9354\n",
            "Epoch 38/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1685 - accuracy: 0.9412 - val_loss: 0.1682 - val_accuracy: 0.9354\n",
            "Epoch 39/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1675 - accuracy: 0.9415 - val_loss: 0.1673 - val_accuracy: 0.9362\n",
            "Epoch 40/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1658 - accuracy: 0.9423 - val_loss: 0.1664 - val_accuracy: 0.9362\n",
            "Epoch 41/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1651 - accuracy: 0.9433 - val_loss: 0.1653 - val_accuracy: 0.9369\n",
            "Epoch 42/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1634 - accuracy: 0.9443 - val_loss: 0.1645 - val_accuracy: 0.9362\n",
            "Epoch 43/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1615 - accuracy: 0.9438 - val_loss: 0.1631 - val_accuracy: 0.9346\n",
            "Epoch 44/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1598 - accuracy: 0.9438 - val_loss: 0.1606 - val_accuracy: 0.9392\n",
            "Epoch 45/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1586 - accuracy: 0.9446 - val_loss: 0.1593 - val_accuracy: 0.9377\n",
            "Epoch 46/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1567 - accuracy: 0.9451 - val_loss: 0.1579 - val_accuracy: 0.9385\n",
            "Epoch 47/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1549 - accuracy: 0.9453 - val_loss: 0.1579 - val_accuracy: 0.9362\n",
            "Epoch 48/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1538 - accuracy: 0.9446 - val_loss: 0.1552 - val_accuracy: 0.9423\n",
            "Epoch 49/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1526 - accuracy: 0.9464 - val_loss: 0.1559 - val_accuracy: 0.9377\n",
            "Epoch 50/2000\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1507 - accuracy: 0.9469 - val_loss: 0.1516 - val_accuracy: 0.9415\n",
            "Epoch 51/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1490 - accuracy: 0.9477 - val_loss: 0.1497 - val_accuracy: 0.9431\n",
            "Epoch 52/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1474 - accuracy: 0.9484 - val_loss: 0.1497 - val_accuracy: 0.9423\n",
            "Epoch 53/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1465 - accuracy: 0.9497 - val_loss: 0.1488 - val_accuracy: 0.9415\n",
            "Epoch 54/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1452 - accuracy: 0.9497 - val_loss: 0.1455 - val_accuracy: 0.9462\n",
            "Epoch 55/2000\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1428 - accuracy: 0.9497 - val_loss: 0.1445 - val_accuracy: 0.9454\n",
            "Epoch 56/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1427 - accuracy: 0.9502 - val_loss: 0.1447 - val_accuracy: 0.9431\n",
            "Epoch 57/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1397 - accuracy: 0.9525 - val_loss: 0.1418 - val_accuracy: 0.9492\n",
            "Epoch 58/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1377 - accuracy: 0.9528 - val_loss: 0.1409 - val_accuracy: 0.9423\n",
            "Epoch 59/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1374 - accuracy: 0.9505 - val_loss: 0.1384 - val_accuracy: 0.9508\n",
            "Epoch 60/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1348 - accuracy: 0.9543 - val_loss: 0.1405 - val_accuracy: 0.9462\n",
            "Epoch 61/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1350 - accuracy: 0.9543 - val_loss: 0.1360 - val_accuracy: 0.9469\n",
            "Epoch 62/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1349 - accuracy: 0.9543 - val_loss: 0.1369 - val_accuracy: 0.9546\n",
            "Epoch 63/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1330 - accuracy: 0.9546 - val_loss: 0.1337 - val_accuracy: 0.9523\n",
            "Epoch 64/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1300 - accuracy: 0.9564 - val_loss: 0.1357 - val_accuracy: 0.9469\n",
            "Epoch 65/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1315 - accuracy: 0.9556 - val_loss: 0.1321 - val_accuracy: 0.9508\n",
            "Epoch 66/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1277 - accuracy: 0.9564 - val_loss: 0.1307 - val_accuracy: 0.9515\n",
            "Epoch 67/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1258 - accuracy: 0.9577 - val_loss: 0.1294 - val_accuracy: 0.9515\n",
            "Epoch 68/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1255 - accuracy: 0.9559 - val_loss: 0.1298 - val_accuracy: 0.9600\n",
            "Epoch 69/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1227 - accuracy: 0.9579 - val_loss: 0.1332 - val_accuracy: 0.9477\n",
            "Epoch 70/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1241 - accuracy: 0.9587 - val_loss: 0.1251 - val_accuracy: 0.9592\n",
            "Epoch 71/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1203 - accuracy: 0.9582 - val_loss: 0.1236 - val_accuracy: 0.9538\n",
            "Epoch 72/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1191 - accuracy: 0.9605 - val_loss: 0.1221 - val_accuracy: 0.9554\n",
            "Epoch 73/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1178 - accuracy: 0.9610 - val_loss: 0.1218 - val_accuracy: 0.9554\n",
            "Epoch 74/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1162 - accuracy: 0.9607 - val_loss: 0.1206 - val_accuracy: 0.9631\n",
            "Epoch 75/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9623 - val_loss: 0.1216 - val_accuracy: 0.9523\n",
            "Epoch 76/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1144 - accuracy: 0.9618 - val_loss: 0.1198 - val_accuracy: 0.9677\n",
            "Epoch 77/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1132 - accuracy: 0.9620 - val_loss: 0.1166 - val_accuracy: 0.9600\n",
            "Epoch 78/2000\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1126 - accuracy: 0.9625 - val_loss: 0.1161 - val_accuracy: 0.9562\n",
            "Epoch 79/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1119 - accuracy: 0.9641 - val_loss: 0.1177 - val_accuracy: 0.9523\n",
            "Epoch 80/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1118 - accuracy: 0.9643 - val_loss: 0.1139 - val_accuracy: 0.9600\n",
            "Epoch 81/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1105 - accuracy: 0.9618 - val_loss: 0.1131 - val_accuracy: 0.9662\n",
            "Epoch 82/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1079 - accuracy: 0.9648 - val_loss: 0.1123 - val_accuracy: 0.9592\n",
            "Epoch 83/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1067 - accuracy: 0.9654 - val_loss: 0.1124 - val_accuracy: 0.9685\n",
            "Epoch 84/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1076 - accuracy: 0.9643 - val_loss: 0.1107 - val_accuracy: 0.9646\n",
            "Epoch 85/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1055 - accuracy: 0.9636 - val_loss: 0.1133 - val_accuracy: 0.9562\n",
            "Epoch 86/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1082 - accuracy: 0.9628 - val_loss: 0.1112 - val_accuracy: 0.9646\n",
            "Epoch 87/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1068 - accuracy: 0.9641 - val_loss: 0.1064 - val_accuracy: 0.9677\n",
            "Epoch 88/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9651 - val_loss: 0.1105 - val_accuracy: 0.9631\n",
            "Epoch 89/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1013 - accuracy: 0.9656 - val_loss: 0.1051 - val_accuracy: 0.9692\n",
            "Epoch 90/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0998 - accuracy: 0.9656 - val_loss: 0.1047 - val_accuracy: 0.9692\n",
            "Epoch 91/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1015 - accuracy: 0.9651 - val_loss: 0.1032 - val_accuracy: 0.9700\n",
            "Epoch 92/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1007 - accuracy: 0.9679 - val_loss: 0.1034 - val_accuracy: 0.9669\n",
            "Epoch 93/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0982 - accuracy: 0.9677 - val_loss: 0.1039 - val_accuracy: 0.9592\n",
            "Epoch 94/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0984 - accuracy: 0.9666 - val_loss: 0.1019 - val_accuracy: 0.9700\n",
            "Epoch 95/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0963 - accuracy: 0.9674 - val_loss: 0.0997 - val_accuracy: 0.9692\n",
            "Epoch 96/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0946 - accuracy: 0.9677 - val_loss: 0.0986 - val_accuracy: 0.9700\n",
            "Epoch 97/2000\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0936 - accuracy: 0.9690 - val_loss: 0.0984 - val_accuracy: 0.9708\n",
            "Epoch 98/2000\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0942 - accuracy: 0.9679 - val_loss: 0.0976 - val_accuracy: 0.9700\n",
            "Epoch 99/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0917 - accuracy: 0.9682 - val_loss: 0.0961 - val_accuracy: 0.9700\n",
            "Epoch 100/2000\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0912 - accuracy: 0.9684 - val_loss: 0.0949 - val_accuracy: 0.9723\n",
            "Epoch 101/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0904 - accuracy: 0.9700 - val_loss: 0.0949 - val_accuracy: 0.9715\n",
            "Epoch 102/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0897 - accuracy: 0.9695 - val_loss: 0.0941 - val_accuracy: 0.9723\n",
            "Epoch 103/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0899 - accuracy: 0.9710 - val_loss: 0.0934 - val_accuracy: 0.9731\n",
            "Epoch 104/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0892 - accuracy: 0.9723 - val_loss: 0.0953 - val_accuracy: 0.9700\n",
            "Epoch 105/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0884 - accuracy: 0.9702 - val_loss: 0.0903 - val_accuracy: 0.9715\n",
            "Epoch 106/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0886 - accuracy: 0.9692 - val_loss: 0.0941 - val_accuracy: 0.9708\n",
            "Epoch 107/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0894 - accuracy: 0.9725 - val_loss: 0.0926 - val_accuracy: 0.9731\n",
            "Epoch 108/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0856 - accuracy: 0.9738 - val_loss: 0.0889 - val_accuracy: 0.9738\n",
            "Epoch 109/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0850 - accuracy: 0.9720 - val_loss: 0.0881 - val_accuracy: 0.9754\n",
            "Epoch 110/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0845 - accuracy: 0.9725 - val_loss: 0.0888 - val_accuracy: 0.9731\n",
            "Epoch 111/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0849 - accuracy: 0.9715 - val_loss: 0.0868 - val_accuracy: 0.9731\n",
            "Epoch 112/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0851 - accuracy: 0.9713 - val_loss: 0.0864 - val_accuracy: 0.9746\n",
            "Epoch 113/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0841 - accuracy: 0.9723 - val_loss: 0.0874 - val_accuracy: 0.9738\n",
            "Epoch 114/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9718 - val_loss: 0.0845 - val_accuracy: 0.9746\n",
            "Epoch 115/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0815 - accuracy: 0.9725 - val_loss: 0.0882 - val_accuracy: 0.9738\n",
            "Epoch 116/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0818 - accuracy: 0.9736 - val_loss: 0.0831 - val_accuracy: 0.9754\n",
            "Epoch 117/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0807 - accuracy: 0.9715 - val_loss: 0.0824 - val_accuracy: 0.9746\n",
            "Epoch 118/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.9759 - val_loss: 0.0839 - val_accuracy: 0.9769\n",
            "Epoch 119/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0804 - accuracy: 0.9746 - val_loss: 0.0839 - val_accuracy: 0.9754\n",
            "Epoch 120/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0789 - accuracy: 0.9749 - val_loss: 0.0816 - val_accuracy: 0.9746\n",
            "Epoch 121/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0787 - accuracy: 0.9751 - val_loss: 0.0820 - val_accuracy: 0.9746\n",
            "Epoch 122/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0787 - accuracy: 0.9756 - val_loss: 0.0807 - val_accuracy: 0.9754\n",
            "Epoch 123/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0781 - accuracy: 0.9738 - val_loss: 0.0820 - val_accuracy: 0.9754\n",
            "Epoch 124/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0761 - accuracy: 0.9756 - val_loss: 0.0837 - val_accuracy: 0.9746\n",
            "Epoch 125/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0793 - accuracy: 0.9746 - val_loss: 0.0804 - val_accuracy: 0.9731\n",
            "Epoch 126/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0790 - accuracy: 0.9733 - val_loss: 0.0787 - val_accuracy: 0.9769\n",
            "Epoch 127/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0803 - accuracy: 0.9751 - val_loss: 0.0835 - val_accuracy: 0.9738\n",
            "Epoch 128/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0746 - accuracy: 0.9777 - val_loss: 0.0774 - val_accuracy: 0.9777\n",
            "Epoch 129/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0768 - accuracy: 0.9751 - val_loss: 0.0768 - val_accuracy: 0.9769\n",
            "Epoch 130/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0741 - accuracy: 0.9779 - val_loss: 0.0774 - val_accuracy: 0.9762\n",
            "Epoch 131/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0733 - accuracy: 0.9772 - val_loss: 0.0774 - val_accuracy: 0.9769\n",
            "Epoch 132/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0733 - accuracy: 0.9766 - val_loss: 0.0768 - val_accuracy: 0.9762\n",
            "Epoch 133/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0728 - accuracy: 0.9761 - val_loss: 0.0757 - val_accuracy: 0.9769\n",
            "Epoch 134/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0718 - accuracy: 0.9769 - val_loss: 0.0753 - val_accuracy: 0.9762\n",
            "Epoch 135/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0712 - accuracy: 0.9782 - val_loss: 0.0746 - val_accuracy: 0.9762\n",
            "Epoch 136/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0719 - accuracy: 0.9784 - val_loss: 0.0765 - val_accuracy: 0.9762\n",
            "Epoch 137/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0746 - accuracy: 0.9756 - val_loss: 0.0742 - val_accuracy: 0.9769\n",
            "Epoch 138/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0732 - accuracy: 0.9800 - val_loss: 0.0800 - val_accuracy: 0.9723\n",
            "Epoch 139/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0709 - accuracy: 0.9779 - val_loss: 0.0730 - val_accuracy: 0.9769\n",
            "Epoch 140/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0697 - accuracy: 0.9782 - val_loss: 0.0733 - val_accuracy: 0.9785\n",
            "Epoch 141/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0695 - accuracy: 0.9792 - val_loss: 0.0720 - val_accuracy: 0.9777\n",
            "Epoch 142/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0694 - accuracy: 0.9782 - val_loss: 0.0728 - val_accuracy: 0.9769\n",
            "Epoch 143/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0691 - accuracy: 0.9782 - val_loss: 0.0716 - val_accuracy: 0.9785\n",
            "Epoch 144/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0691 - accuracy: 0.9782 - val_loss: 0.0712 - val_accuracy: 0.9777\n",
            "Epoch 145/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0688 - accuracy: 0.9795 - val_loss: 0.0710 - val_accuracy: 0.9769\n",
            "Epoch 146/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0663 - accuracy: 0.9787 - val_loss: 0.0703 - val_accuracy: 0.9777\n",
            "Epoch 147/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0677 - accuracy: 0.9774 - val_loss: 0.0706 - val_accuracy: 0.9762\n",
            "Epoch 148/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0673 - accuracy: 0.9797 - val_loss: 0.0702 - val_accuracy: 0.9762\n",
            "Epoch 149/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0663 - accuracy: 0.9795 - val_loss: 0.0692 - val_accuracy: 0.9777\n",
            "Epoch 150/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0667 - accuracy: 0.9792 - val_loss: 0.0691 - val_accuracy: 0.9785\n",
            "Epoch 151/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0659 - accuracy: 0.9815 - val_loss: 0.0704 - val_accuracy: 0.9769\n",
            "Epoch 152/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0653 - accuracy: 0.9805 - val_loss: 0.0693 - val_accuracy: 0.9754\n",
            "Epoch 153/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0671 - accuracy: 0.9790 - val_loss: 0.0695 - val_accuracy: 0.9762\n",
            "Epoch 154/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0651 - accuracy: 0.9800 - val_loss: 0.0707 - val_accuracy: 0.9754\n",
            "Epoch 155/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0668 - accuracy: 0.9795 - val_loss: 0.0671 - val_accuracy: 0.9800\n",
            "Epoch 156/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0660 - accuracy: 0.9802 - val_loss: 0.0669 - val_accuracy: 0.9800\n",
            "Epoch 157/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0647 - accuracy: 0.9790 - val_loss: 0.0697 - val_accuracy: 0.9777\n",
            "Epoch 158/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0635 - accuracy: 0.9810 - val_loss: 0.0673 - val_accuracy: 0.9785\n",
            "Epoch 159/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0644 - accuracy: 0.9810 - val_loss: 0.0683 - val_accuracy: 0.9769\n",
            "Epoch 160/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0634 - accuracy: 0.9810 - val_loss: 0.0668 - val_accuracy: 0.9792\n",
            "Epoch 161/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0673 - accuracy: 0.9790 - val_loss: 0.0733 - val_accuracy: 0.9746\n",
            "Epoch 162/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0682 - accuracy: 0.9784 - val_loss: 0.0681 - val_accuracy: 0.9762\n",
            "Epoch 163/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0677 - accuracy: 0.9787 - val_loss: 0.0666 - val_accuracy: 0.9754\n",
            "Epoch 164/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0632 - accuracy: 0.9795 - val_loss: 0.0712 - val_accuracy: 0.9777\n",
            "Epoch 165/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0660 - accuracy: 0.9810 - val_loss: 0.0727 - val_accuracy: 0.9762\n",
            "Epoch 166/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0628 - accuracy: 0.9808 - val_loss: 0.0685 - val_accuracy: 0.9785\n",
            "Epoch 167/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0635 - accuracy: 0.9797 - val_loss: 0.0655 - val_accuracy: 0.9792\n",
            "Epoch 168/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0612 - accuracy: 0.9820 - val_loss: 0.0673 - val_accuracy: 0.9746\n",
            "Epoch 169/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0625 - accuracy: 0.9818 - val_loss: 0.0676 - val_accuracy: 0.9785\n",
            "Epoch 170/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0624 - accuracy: 0.9810 - val_loss: 0.0654 - val_accuracy: 0.9792\n",
            "Epoch 171/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0627 - accuracy: 0.9805 - val_loss: 0.0657 - val_accuracy: 0.9777\n",
            "Epoch 172/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0606 - accuracy: 0.9813 - val_loss: 0.0666 - val_accuracy: 0.9785\n",
            "Epoch 173/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0605 - accuracy: 0.9815 - val_loss: 0.0646 - val_accuracy: 0.9785\n",
            "Epoch 174/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0593 - accuracy: 0.9823 - val_loss: 0.0642 - val_accuracy: 0.9785\n",
            "Epoch 175/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0613 - accuracy: 0.9818 - val_loss: 0.0664 - val_accuracy: 0.9777\n",
            "Epoch 176/2000\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0589 - accuracy: 0.9828 - val_loss: 0.0634 - val_accuracy: 0.9792\n",
            "Epoch 177/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0690 - val_accuracy: 0.9792\n",
            "Epoch 178/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0600 - accuracy: 0.9818 - val_loss: 0.0672 - val_accuracy: 0.9785\n",
            "Epoch 179/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0594 - accuracy: 0.9823 - val_loss: 0.0644 - val_accuracy: 0.9777\n",
            "Epoch 180/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9818 - val_loss: 0.0650 - val_accuracy: 0.9785\n",
            "Epoch 181/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0577 - accuracy: 0.9836 - val_loss: 0.0665 - val_accuracy: 0.9785\n",
            "Epoch 182/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0608 - accuracy: 0.9813 - val_loss: 0.0630 - val_accuracy: 0.9769\n",
            "Epoch 183/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0622 - accuracy: 0.9805 - val_loss: 0.0612 - val_accuracy: 0.9808\n",
            "Epoch 184/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0593 - accuracy: 0.9818 - val_loss: 0.0636 - val_accuracy: 0.9777\n",
            "Epoch 185/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0569 - accuracy: 0.9836 - val_loss: 0.0647 - val_accuracy: 0.9769\n",
            "Epoch 186/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.9826 - val_loss: 0.0623 - val_accuracy: 0.9792\n",
            "Epoch 187/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0576 - accuracy: 0.9823 - val_loss: 0.0623 - val_accuracy: 0.9777\n",
            "Epoch 188/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0552 - accuracy: 0.9836 - val_loss: 0.0636 - val_accuracy: 0.9777\n",
            "Epoch 189/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0570 - accuracy: 0.9836 - val_loss: 0.0634 - val_accuracy: 0.9769\n",
            "Epoch 190/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0560 - accuracy: 0.9836 - val_loss: 0.0621 - val_accuracy: 0.9785\n",
            "Epoch 191/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0570 - accuracy: 0.9826 - val_loss: 0.0619 - val_accuracy: 0.9785\n",
            "Epoch 192/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0557 - accuracy: 0.9828 - val_loss: 0.0630 - val_accuracy: 0.9808\n",
            "Epoch 193/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0558 - accuracy: 0.9846 - val_loss: 0.0630 - val_accuracy: 0.9777\n",
            "Epoch 194/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0554 - accuracy: 0.9826 - val_loss: 0.0614 - val_accuracy: 0.9785\n",
            "Epoch 195/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0616 - val_accuracy: 0.9769\n",
            "Epoch 196/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0553 - accuracy: 0.9831 - val_loss: 0.0651 - val_accuracy: 0.9746\n",
            "Epoch 197/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0555 - accuracy: 0.9838 - val_loss: 0.0619 - val_accuracy: 0.9785\n",
            "Epoch 198/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0549 - accuracy: 0.9838 - val_loss: 0.0612 - val_accuracy: 0.9792\n",
            "Epoch 199/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0535 - accuracy: 0.9836 - val_loss: 0.0607 - val_accuracy: 0.9777\n",
            "Epoch 200/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0542 - accuracy: 0.9841 - val_loss: 0.0615 - val_accuracy: 0.9769\n",
            "Epoch 201/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0553 - accuracy: 0.9838 - val_loss: 0.0665 - val_accuracy: 0.9746\n",
            "Epoch 202/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0575 - accuracy: 0.9823 - val_loss: 0.0606 - val_accuracy: 0.9792\n",
            "Epoch 203/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0603 - val_accuracy: 0.9785\n",
            "Epoch 204/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0526 - accuracy: 0.9836 - val_loss: 0.0607 - val_accuracy: 0.9800\n",
            "Epoch 205/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0529 - accuracy: 0.9836 - val_loss: 0.0596 - val_accuracy: 0.9777\n",
            "Epoch 206/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0533 - accuracy: 0.9846 - val_loss: 0.0595 - val_accuracy: 0.9808\n",
            "Epoch 207/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0529 - accuracy: 0.9841 - val_loss: 0.0639 - val_accuracy: 0.9777\n",
            "Epoch 208/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0538 - accuracy: 0.9859 - val_loss: 0.0607 - val_accuracy: 0.9777\n",
            "Epoch 209/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0535 - accuracy: 0.9841 - val_loss: 0.0593 - val_accuracy: 0.9792\n",
            "Epoch 210/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0521 - accuracy: 0.9846 - val_loss: 0.0598 - val_accuracy: 0.9800\n",
            "Epoch 211/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9841 - val_loss: 0.0614 - val_accuracy: 0.9792\n",
            "Epoch 212/2000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0521 - accuracy: 0.9849 - val_loss: 0.0587 - val_accuracy: 0.9792\n",
            "Epoch 213/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9836 - val_loss: 0.0591 - val_accuracy: 0.9800\n",
            "Epoch 214/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0516 - accuracy: 0.9859 - val_loss: 0.0593 - val_accuracy: 0.9785\n",
            "Epoch 215/2000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0529 - accuracy: 0.9856 - val_loss: 0.0583 - val_accuracy: 0.9808\n",
            "Epoch 216/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0531 - accuracy: 0.9836 - val_loss: 0.0582 - val_accuracy: 0.9808\n",
            "Epoch 217/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9838 - val_loss: 0.0600 - val_accuracy: 0.9785\n",
            "Epoch 218/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0510 - accuracy: 0.9856 - val_loss: 0.0601 - val_accuracy: 0.9808\n",
            "Epoch 219/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0515 - accuracy: 0.9856 - val_loss: 0.0594 - val_accuracy: 0.9800\n",
            "Epoch 220/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0503 - accuracy: 0.9851 - val_loss: 0.0590 - val_accuracy: 0.9800\n",
            "Epoch 221/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0528 - accuracy: 0.9859 - val_loss: 0.0635 - val_accuracy: 0.9785\n",
            "Epoch 222/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9856 - val_loss: 0.0593 - val_accuracy: 0.9800\n",
            "Epoch 223/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0515 - accuracy: 0.9846 - val_loss: 0.0607 - val_accuracy: 0.9792\n",
            "Epoch 224/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0502 - accuracy: 0.9859 - val_loss: 0.0602 - val_accuracy: 0.9777\n",
            "Epoch 225/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9856 - val_loss: 0.0583 - val_accuracy: 0.9808\n",
            "Epoch 226/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0506 - accuracy: 0.9846 - val_loss: 0.0582 - val_accuracy: 0.9808\n",
            "Epoch 227/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0497 - accuracy: 0.9851 - val_loss: 0.0579 - val_accuracy: 0.9808\n",
            "Epoch 228/2000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0505 - accuracy: 0.9859 - val_loss: 0.0593 - val_accuracy: 0.9815\n",
            "Epoch 229/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0540 - accuracy: 0.9838 - val_loss: 0.0717 - val_accuracy: 0.9746\n",
            "Epoch 230/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0517 - accuracy: 0.9841 - val_loss: 0.0586 - val_accuracy: 0.9808\n",
            "Epoch 231/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0493 - accuracy: 0.9854 - val_loss: 0.0595 - val_accuracy: 0.9808\n",
            "Epoch 232/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0495 - accuracy: 0.9861 - val_loss: 0.0601 - val_accuracy: 0.9792\n",
            "Epoch 233/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0507 - accuracy: 0.9856 - val_loss: 0.0579 - val_accuracy: 0.9815\n",
            "Epoch 234/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9854 - val_loss: 0.0590 - val_accuracy: 0.9800\n",
            "Epoch 235/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9867 - val_loss: 0.0578 - val_accuracy: 0.9815\n",
            "Epoch 236/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0486 - accuracy: 0.9859 - val_loss: 0.0596 - val_accuracy: 0.9800\n",
            "Epoch 237/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0484 - accuracy: 0.9849 - val_loss: 0.0580 - val_accuracy: 0.9815\n",
            "Epoch 238/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.9861 - val_loss: 0.0585 - val_accuracy: 0.9800\n",
            "Epoch 239/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0487 - accuracy: 0.9861 - val_loss: 0.0581 - val_accuracy: 0.9815\n",
            "Epoch 240/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0485 - accuracy: 0.9851 - val_loss: 0.0569 - val_accuracy: 0.9808\n",
            "Epoch 241/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0479 - accuracy: 0.9859 - val_loss: 0.0575 - val_accuracy: 0.9808\n",
            "Epoch 242/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0480 - accuracy: 0.9859 - val_loss: 0.0575 - val_accuracy: 0.9808\n",
            "Epoch 243/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0481 - accuracy: 0.9867 - val_loss: 0.0585 - val_accuracy: 0.9808\n",
            "Epoch 244/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.9854 - val_loss: 0.0578 - val_accuracy: 0.9808\n",
            "Epoch 245/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0480 - accuracy: 0.9872 - val_loss: 0.0589 - val_accuracy: 0.9808\n",
            "Epoch 246/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0471 - accuracy: 0.9864 - val_loss: 0.0583 - val_accuracy: 0.9823\n",
            "Epoch 247/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0491 - accuracy: 0.9854 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
            "Epoch 248/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0474 - accuracy: 0.9859 - val_loss: 0.0583 - val_accuracy: 0.9815\n",
            "Epoch 249/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0473 - accuracy: 0.9867 - val_loss: 0.0595 - val_accuracy: 0.9808\n",
            "Epoch 250/2000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0497 - accuracy: 0.9861 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
            "Epoch 251/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0571 - val_accuracy: 0.9831\n",
            "Epoch 252/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0492 - accuracy: 0.9861 - val_loss: 0.0590 - val_accuracy: 0.9831\n",
            "Epoch 253/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0513 - accuracy: 0.9851 - val_loss: 0.0601 - val_accuracy: 0.9792\n",
            "Epoch 254/2000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0516 - accuracy: 0.9851 - val_loss: 0.0582 - val_accuracy: 0.9823\n",
            "Epoch 255/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0488 - accuracy: 0.9851 - val_loss: 0.0578 - val_accuracy: 0.9831\n",
            "Epoch 256/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.9864 - val_loss: 0.0592 - val_accuracy: 0.9808\n",
            "Epoch 257/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0464 - accuracy: 0.9874 - val_loss: 0.0649 - val_accuracy: 0.9769\n",
            "Epoch 258/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0473 - accuracy: 0.9864 - val_loss: 0.0571 - val_accuracy: 0.9831\n",
            "Epoch 259/2000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0466 - accuracy: 0.9864 - val_loss: 0.0608 - val_accuracy: 0.9800\n",
            "Epoch 260/2000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0477 - accuracy: 0.9885 - val_loss: 0.0630 - val_accuracy: 0.9785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score=model.evaluate(X_test,y_test)\n",
        "print('Test accuracy:',score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNSc5K1MpL3K",
        "outputId": "074485ce-e21b-4c52-b322-cce138a41aa5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9862\n",
            "Test accuracy: 0.9861538410186768\n"
          ]
        }
      ]
    }
  ]
}